{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Rectangle Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape: (50000, 8, 8)\n",
      "Bounding Box Shape: (50000, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "# Create images with random rectangles and bounding boxes. \n",
    "num_imgs = 50000\n",
    "\n",
    "img_size = 8\n",
    "min_object_size = 1\n",
    "max_object_size = 4\n",
    "num_objects = 1\n",
    "\n",
    "# create and zero two arrays, one is for the bounding boxes, the other is for the images\n",
    "bboxes = np.zeros((num_imgs, num_objects, 4))\n",
    "# set the pixel value of the background to 0\n",
    "imgs = np.zeros((num_imgs, img_size, img_size))\n",
    "\n",
    "for i_img in range(num_imgs):\n",
    "    for i_object in range(num_objects):\n",
    "        # set the width and height of the rectangle\n",
    "        w, h = np.random.randint(min_object_size, max_object_size, size = 2)\n",
    "        # set the location (x, y) of the rectangle\n",
    "        x = np.random.randint(0, img_size - w)\n",
    "        y = np.random.randint(0, img_size - h)\n",
    "        # set the pixel value of the rectangle to 1\n",
    "        imgs[i_img, x:x + w, y:y + h] = 1.0\n",
    "        bboxes[i_img, i_object] = [x, y, w, h]\n",
    "        \n",
    "print(\"Image Shape: {}\".format(imgs.shape))\n",
    "print(\"Bounding Box Shape: {}\".format(bboxes.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAKeElEQVR4nO3df6hndZ3H8edrZwydKTFaWWpGsCC0EEr3YuaAsI676BbuP8GOYFAE809bGkHk/hP7f0T9EcGgtQu6xu6osMiua5ASwja713F2U69B2ayO2s4Mi2m2ZNa7P+7XkGma75m+59zvve99PuDL3B/nC+8vc59zzvfcM+eTqkJST3+w7AEkTcfApcYMXGrMwKXGDFxqzMClxgYFnuQzSZ5I8niSu5OcO/VgkhY3N/Aku4BPAytVdRmwDdg39WCSFjf0EH07cF6S7cAO4PnpRpI0lu3zNqiq55J8EXgG+D/gwap68NTtkuwH9gPs3Lnzjy+99NKxZ5U0c/ToUU6ePJl522XepapJ3grcA/wl8CLwj8DBqrrzdz1nZWWlVldXz25iSYOtrKywuro6N/Ahh+jXAT+qqhNV9QvgXuDqRQeUNL0hgT8DXJVkR5IAe4G1aceSNIa5gVfVIeAgcBj43uw5ByaeS9II5p5kA6iqLwBfmHgWSSPzSjapMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5caG7KyySVJjrzh8VKSWzdiOEmLGbLwwfeB9wMk2QY8B9w38VySRnC2h+h7gR9W1X9PMYykcZ1t4PuAu6cYRNL4Bgee5E3AjawvXXS67+9Psppk9cSJE2PNJ2kBZ7MHvwE4XFX/c7pvVtWBqlqpqpULL7xwnOkkLeRsAr8JD8+lLWVQ4El2AH/K+sKDkraIoUsX/Qx428SzSBqZV7JJjRm41JiBS40ZuNSYgUuNGbjUmIFLjRm41JiBS40ZuNSYgUuNGbjUmIFLjRm41JiBS40ZuNSYgUuNGbjUmIFLjQ296eIFSQ4meSrJWpIPTj2YpMUNuuki8BXggar6yGwBhB0TziRpJHMDT3I+cA3wMYCqehV4ddqxJI1hyCH6u4ATwDeSPJbk9iQ7T93IpYukzWdI4NuBK4CvVdXlwCvA50/dyKWLpM1nSODHgGNVdWj2+UHWg5e0yc0NvKp+DDyb5JLZl/YCT046laRRDD2L/ingrtkZ9KeBj083kqSxDF2b7AiwMvEskkbmlWxSYwYuNWbgUmMGLjVm4FJjBi41ZuBSYwYuNWbgUmMGLjVm4FJjBi41ZuBSYwYuNWbgUmMGLjVm4FJjBi41NuiWTUmOAi8DvwReqypv3yRtAUNvugjwJ1V1crJJJI3OQ3SpsaGBF/BgkkeT7D/dBi5dJG0+QwPfU1VXADcAn0xyzakbuHSRtPkMCryqnp/9eRy4D7hyyqEkjWNu4El2JnnL6x8DfwY8PvVgkhY35Cz6HwH3JXl9+7+vqgcmnUrSKOYGXlVPA+/bgFkkjcxfk0mNGbjUmIFLjRm41JiBS40ZuNSYgUuNGbjUmIFLjRm41JiBS40ZuNSYgUuNGbjUmIFLjRm41JiBS40ZuNTY4MCTbEvyWJL7pxxI0njOZg9+C7A21SCSxjco8CS7gQ8Bt087jjSiZOMfm8zQPfiXgc8Bv/pdG7h0kbT5DFn44MPA8ap69EzbuXSRNqtswGOzGrIH3wPcOFsj/JvAtUnunHQqSaOYG3hV3VZVu6vqYmAf8O2qunnyySQtzN+DS40NWZvsN6rqYeDhSSaRNDr34FJjBi41ZuBSYwYuNWbgUmMGLjVm4FJjBi41ZuBSYwYuNWbgUmMGLjVm4FJjBi41ZuBSYwYuNWbgUmMGLjU25LbJ5yb59yT/meSJJH+zEYNJWtyQe7L9HLi2qn6a5BzgkST/UlXfnXg2SQuaG3hVFfDT2afnzB415VCSxjF0bbJtSY4Ax4FvVdWh02zj0kXSJjMo8Kr6ZVW9H9gNXJnkstNs49JF0iZzVmfRq+pF1u+Lfv0k00ga1ZCz6BcmuWD28XnAdcBTUw8maXFDzqK/Hfi7JNtY/wfhH6rq/mnHkjSGIWfR/wu4fANmkTQyr2STGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpsSE3XbwoyUNJ1mZLF92yEYNJWtyQmy6+Bny2qg4neQvwaJJvVdWTE88maUFDbrr4AvDC7OOXk6wBuwAD15bw/3mdrbN6D57kYtbvsOrSRdIWMDjwJG8G7gFuraqXTv2+Sxdp06na+McmM3TxwXNYj/uuqrp32pEkjWXIWfQAdwBrVfWl6UeSNJYhe/A9wEeBa5McmT3+fOK5JI1gyFn0R4BswCySRuaVbFJjBi41ZuBSYwYuNWbgUmMGLjVm4FJjBi41ZuBSYwYuNWbgUmMGLjVm4FJjBi41ZuBSYwYuNWbgUmMGLjU25KaLX09yPMnjGzGQpPEM2YP/LXD9xHNImsDcwKvqO8D/bsAskkbme3CpsdECd20yafMZLXDXJpM2Hw/RpcaG/JrsbuDfgEuSHEvyienHkjSGIUsX3bQRg0gan4foUmMGLjVm4FJjBi41ZuBSYwYuNWbgUmMGLjVm4FJjBi41ZuBSYwYuNWbgUmMGLjVm4FJjBi41ZuBSYwYuNWbgUmODAk9yfZLvJ/lBks9PPZSkcQy5q+o24KvADcB7gZuSvHfqwSQtbsge/ErgB1X1dFW9CnwT+Itpx5I0hrm3TQZ2Ac++4fNjwAdO3SjJfmD/7NOfN11u+A+Bk8seYgJdXxf0fW2XDNloSOA5zdfqt75QdQA4AJBktapWhgywlfi6tp6ury3J6pDthhyiHwMuesPnu4Hnf5+hJG2sIYH/B/DuJO9M8iZgH/BP044laQxDli56LclfAf8KbAO+XlVPzHnagTGG24R8XVtP19c26HWl6rfeTktqwivZpMYMXGps1MA7XtKa5KIkDyVZS/JEkluWPdPYkmxL8liS+5c9y1iSXJDkYJKnZn93H1z2TGNI8pnZz+HjSe5Ocu6Zth8t8MaXtL4GfLaq3gNcBXyyyet6o1uAtWUPMbKvAA9U1aXA+2jw+pLsAj4NrFTVZayf9N53pueMuQdveUlrVb1QVYdnH7/M+g/KruVONZ4ku4EPAbcve5axJDkfuAa4A6CqXq2qF5c71Wi2A+cl2Q7sYM41KWMGfrpLWtuEAJDkYuBy4NByJxnVl4HPAb9a9iAjehdwAvjG7K3H7Ul2LnuoRVXVc8AXgWeAF4CfVNWDZ3rOmIEPuqR1q0ryZuAe4NaqemnZ84whyYeB41X16LJnGdl24Arga1V1OfAKsOXPCSV5K+tHxe8E3gHsTHLzmZ4zZuBtL2lNcg7rcd9VVfcue54R7QFuTHKU9bdU1ya5c7kjjeIYcKyqXj/SOsh68FvddcCPqupEVf0CuBe4+kxPGDPwlpe0Jgnr7+XWqupLy55nTFV1W1XtrqqLWf/7+nZVnXGPsBVU1Y+BZ5O8/j+u9gJPLnGksTwDXJVkx+znci9zTh4O+d9kg/yel7RuBXuAjwLfS3Jk9rW/rqp/XuJMmu9TwF2znc3TwMeXPM/CqupQkoPAYdZ/u/MYcy5Z9VJVqTGvZJMaM3CpMQOXGjNwqTEDlxozcKkxA5ca+zX5g9xH8rEypwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 0\n",
    "# origin: place the [0, 0] index of the array in the upper left or lower left corner of the axes\n",
    "# extent: floats(left, right, bottom, top)\n",
    "plt.imshow(imgs[i].T, cmap = 'Greys', interpolation = 'none', origin = 'lower', extent = [0, img_size, 0, img_size])\n",
    "# tranverse(.T) and flip upside down(origin = 'lower') to fit in the bounding box\n",
    "for bbox in bboxes[i]:\n",
    "    plt.gca().add_patch(matplotlib.patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], lw = 2, ec = 'r', fc = 'none'))\n",
    "    # patches.Rectangle((x, y), w, h, linewidth, edgecolor, facecolor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 64), 4.933831121434196e-17, 1.0000000000000002)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape and normalize the image data to mean 0 and std 1. \n",
    "X = (imgs.reshape(num_imgs, -1) - np.mean(imgs)) / np.std(imgs)\n",
    "X.shape, np.mean(X), np.std(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 4), 0.281475, 0.17526750804127955)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize x, y, w, h by img_size, so that all values are between 0 and 1.\n",
    "# Important: Do not shift to negative values (e.g. by setting to mean 0), because the IOU calculation needs positive w and h.\n",
    "y = bboxes.reshape(num_imgs, -1) / img_size\n",
    "y.shape, np.mean(y), np.std(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and test.\n",
    "i = int(0.8 * num_imgs)\n",
    "train_X, test_X = X[:i], X[i:]\n",
    "train_y ,test_y = y[:i], y[i:]\n",
    "test_imgs = imgs[i:]\n",
    "test_bboxes = bboxes[i:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 200)               13000     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 804       \n",
      "=================================================================\n",
      "Total params: 13,804\n",
      "Trainable params: 13,804\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "model = Sequential([\n",
    "        Dense(200, input_dim = X.shape[-1]), \n",
    "        Activation('relu'), \n",
    "        Dropout(0.2), \n",
    "        Dense(y.shape[-1])\n",
    "        ])\n",
    "model.compile(loss = 'mse', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1250/1250 [==============================] - 1s 883us/step - loss: 0.0260 - accuracy: 0.7088 - val_loss: 7.8385e-04 - val_accuracy: 0.8944\n",
      "Epoch 2/30\n",
      "1250/1250 [==============================] - 1s 783us/step - loss: 0.0023 - accuracy: 0.8438 - val_loss: 2.9428e-04 - val_accuracy: 0.8753\n",
      "Epoch 3/30\n",
      "1250/1250 [==============================] - 1s 787us/step - loss: 0.0014 - accuracy: 0.8486 - val_loss: 1.6840e-04 - val_accuracy: 0.8721\n",
      "Epoch 4/30\n",
      "1250/1250 [==============================] - 1s 781us/step - loss: 0.0011 - accuracy: 0.8478 - val_loss: 1.6911e-04 - val_accuracy: 0.8322\n",
      "Epoch 5/30\n",
      "1250/1250 [==============================] - 1s 785us/step - loss: 9.4217e-04 - accuracy: 0.8485 - val_loss: 2.1356e-04 - val_accuracy: 0.8855\n",
      "Epoch 6/30\n",
      "1250/1250 [==============================] - 1s 796us/step - loss: 8.4843e-04 - accuracy: 0.8477 - val_loss: 1.7260e-04 - val_accuracy: 0.8604\n",
      "Epoch 7/30\n",
      "1250/1250 [==============================] - 1s 850us/step - loss: 7.9675e-04 - accuracy: 0.8481 - val_loss: 2.1689e-04 - val_accuracy: 0.8247\n",
      "Epoch 8/30\n",
      "1250/1250 [==============================] - 1s 798us/step - loss: 7.5871e-04 - accuracy: 0.8460 - val_loss: 1.5603e-04 - val_accuracy: 0.8957\n",
      "Epoch 9/30\n",
      "1250/1250 [==============================] - 1s 783us/step - loss: 7.2384e-04 - accuracy: 0.8458 - val_loss: 1.4723e-04 - val_accuracy: 0.8149\n",
      "Epoch 10/30\n",
      "1250/1250 [==============================] - 1s 780us/step - loss: 6.9962e-04 - accuracy: 0.8449 - val_loss: 1.8782e-04 - val_accuracy: 0.8821\n",
      "Epoch 11/30\n",
      "1250/1250 [==============================] - 1s 787us/step - loss: 6.7483e-04 - accuracy: 0.8440 - val_loss: 1.6433e-04 - val_accuracy: 0.8681\n",
      "Epoch 12/30\n",
      "1250/1250 [==============================] - 1s 868us/step - loss: 6.6396e-04 - accuracy: 0.8460 - val_loss: 1.3507e-04 - val_accuracy: 0.9025\n",
      "Epoch 13/30\n",
      "1250/1250 [==============================] - 1s 786us/step - loss: 6.4136e-04 - accuracy: 0.8471 - val_loss: 1.5583e-04 - val_accuracy: 0.7690\n",
      "Epoch 14/30\n",
      "1250/1250 [==============================] - 1s 783us/step - loss: 6.4202e-04 - accuracy: 0.8441 - val_loss: 1.8472e-04 - val_accuracy: 0.8455\n",
      "Epoch 15/30\n",
      "1250/1250 [==============================] - 1s 786us/step - loss: 6.3617e-04 - accuracy: 0.8450 - val_loss: 1.4767e-04 - val_accuracy: 0.8408\n",
      "Epoch 16/30\n",
      "1250/1250 [==============================] - 1s 786us/step - loss: 6.2387e-04 - accuracy: 0.8423 - val_loss: 1.2489e-04 - val_accuracy: 0.8794\n",
      "Epoch 17/30\n",
      "1250/1250 [==============================] - 1s 790us/step - loss: 6.2030e-04 - accuracy: 0.8457 - val_loss: 1.2101e-04 - val_accuracy: 0.8418\n",
      "Epoch 18/30\n",
      "1250/1250 [==============================] - 1s 797us/step - loss: 6.1403e-04 - accuracy: 0.8442 - val_loss: 1.3876e-04 - val_accuracy: 0.8647\n",
      "Epoch 19/30\n",
      "1250/1250 [==============================] - 1s 862us/step - loss: 6.1252e-04 - accuracy: 0.8446 - val_loss: 1.5315e-04 - val_accuracy: 0.8532\n",
      "Epoch 20/30\n",
      "1250/1250 [==============================] - 1s 804us/step - loss: 6.0236e-04 - accuracy: 0.8443 - val_loss: 1.5798e-04 - val_accuracy: 0.8758\n",
      "Epoch 21/30\n",
      "1250/1250 [==============================] - 1s 824us/step - loss: 6.0480e-04 - accuracy: 0.8432 - val_loss: 1.4356e-04 - val_accuracy: 0.8213\n",
      "Epoch 22/30\n",
      "1250/1250 [==============================] - 1s 832us/step - loss: 6.0359e-04 - accuracy: 0.8450 - val_loss: 1.5483e-04 - val_accuracy: 0.8857\n",
      "Epoch 23/30\n",
      "1250/1250 [==============================] - 1s 844us/step - loss: 5.9842e-04 - accuracy: 0.8440 - val_loss: 1.1426e-04 - val_accuracy: 0.8725\n",
      "Epoch 24/30\n",
      "1250/1250 [==============================] - 1s 786us/step - loss: 5.9508e-04 - accuracy: 0.8463 - val_loss: 1.1130e-04 - val_accuracy: 0.8416\n",
      "Epoch 25/30\n",
      "1250/1250 [==============================] - 1s 790us/step - loss: 5.8894e-04 - accuracy: 0.8450 - val_loss: 1.5980e-04 - val_accuracy: 0.8763\n",
      "Epoch 26/30\n",
      "1250/1250 [==============================] - 1s 785us/step - loss: 5.8735e-04 - accuracy: 0.8444 - val_loss: 9.0072e-05 - val_accuracy: 0.8562\n",
      "Epoch 27/30\n",
      "1250/1250 [==============================] - 1s 797us/step - loss: 5.9452e-04 - accuracy: 0.8444 - val_loss: 1.4270e-04 - val_accuracy: 0.8320\n",
      "Epoch 28/30\n",
      "1250/1250 [==============================] - 1s 831us/step - loss: 5.9434e-04 - accuracy: 0.8435 - val_loss: 1.5426e-04 - val_accuracy: 0.8295\n",
      "Epoch 29/30\n",
      "1250/1250 [==============================] - 1s 913us/step - loss: 5.8904e-04 - accuracy: 0.8447 - val_loss: 1.4293e-04 - val_accuracy: 0.8209\n",
      "Epoch 30/30\n",
      "1250/1250 [==============================] - 1s 882us/step - loss: 5.8940e-04 - accuracy: 0.8442 - val_loss: 1.1991e-04 - val_accuracy: 0.8594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ed4a918820>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model.fit(train_X, train_y, epochs = 30, validation_data = (test_X, test_y), verbose = 1,\n",
    "          callbacks = [ModelCheckpoint('Single_Rectangle_model.h5',\n",
    "                                       monitor = 'val_accuracy',\n",
    "                                       save_best_only = True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict bounding boxes on the test images.\n",
    "from keras.models import load_model\n",
    "model_test = load_model('Single_Rectangle_model.h5')\n",
    "prediction = model_test.predict(test_X)\n",
    "pred_bboxes = prediction * img_size\n",
    "pred_bboxes = pred_bboxes.reshape(len(pred_bboxes), num_objects, -1)\n",
    "pred_bboxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IOU(bbox1, bbox2):\n",
    "    '''Calculate overlap between two bounding boxes [x, y, w, h] as the area of intersection over the area of unity'''\n",
    "    x1, y1, w1, h1 = bbox1[0], bbox1[1], bbox1[2], bbox1[3]\n",
    "    x2, y2, w2, h2 = bbox2[0], bbox2[1], bbox2[2], bbox2[3]\n",
    "\n",
    "    w_I = min(x1 + w1, x2 + w2) - max(x1, x2)\n",
    "    h_I = min(y1 + h1, y2 + h2) - max(y1, y2)\n",
    "    if w_I <= 0 or h_I <= 0:  # no overlap\n",
    "        return 0.\n",
    "    I = w_I * h_I\n",
    "\n",
    "    U = w1 * h1 + w2 * h2 - I\n",
    "\n",
    "    return I / U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x216 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show a few images and predicted bounding boxes from the test dataset. \n",
    "plt.figure(figsize = (12, 3))\n",
    "for i_subplot in range(1, 5):\n",
    "    plt.subplot(1, 4, i_subplot)\n",
    "    i = np.random.randint(len(test_imgs))\n",
    "    plt.imshow(test_imgs[i].T, cmap = 'Greys', interpolation = 'none', origin = 'lower', extent = [0, img_size, 0, img_size])\n",
    "    for pred_bbox, exp_bbox in zip(pred_bboxes[i], test_bboxes[i]):\n",
    "        plt.gca().add_patch(matplotlib.patches.Rectangle((pred_bbox[0], pred_bbox[1]), pred_bbox[2], pred_bbox[3], ec = 'r', fc = 'none'))\n",
    "        plt.annotate(\"IOU: {:.2f}\".format(IOU(pred_bbox, exp_bbox)), (pred_bbox[0], pred_bbox[1] + pred_bbox[3] + 0.2), color = 'r')\n",
    "        # plt.annotate(text, (x, y), color)\n",
    "        \n",
    "plt.savefig(\"single-rectangle_prediction.png\", dpi = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8310774643257784"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean IOU (overlap) between the predicted and expected bounding boxes on the test dataset. \n",
    "summed_IOU = 0.\n",
    "for pred_bbox, test_bbox in zip(pred_bboxes.reshape(-1, 4), test_bboxes.reshape(-1, 4)):\n",
    "    summed_IOU += IOU(pred_bbox, test_bbox)\n",
    "mean_IOU = summed_IOU / len(pred_bboxes)\n",
    "mean_IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
