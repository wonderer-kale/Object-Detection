{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Rectangle Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape: (50000, 8, 8)\n",
      "Bounding Box Shape: (50000, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "# Create images with random rectangles and bounding boxes. \n",
    "num_imgs = 50000\n",
    "\n",
    "img_size = 8\n",
    "min_object_size = 1\n",
    "max_object_size = 4\n",
    "num_objects = 1\n",
    "\n",
    "# create and zero two arrays, one is for the bounding boxes, the other is for the images\n",
    "bboxes = np.zeros((num_imgs, num_objects, 4))\n",
    "# set the pixel value of the background to 0\n",
    "imgs = np.zeros((num_imgs, img_size, img_size))\n",
    "\n",
    "for i_img in range(num_imgs):\n",
    "    for i_object in range(num_objects):\n",
    "        # set the width and height of the rectangle\n",
    "        w, h = np.random.randint(min_object_size, max_object_size, size = 2)\n",
    "        # set the location (x, y) of the rectangle\n",
    "        x = np.random.randint(0, img_size - w)\n",
    "        y = np.random.randint(0, img_size - h)\n",
    "        # set the pixel value of the rectangle to 1\n",
    "        imgs[i_img, x:x + w, y:y + h] = 1.0\n",
    "        bboxes[i_img, i_object] = [x, y, w, h]\n",
    "        \n",
    "print(\"Image Shape: {}\".format(imgs.shape))\n",
    "print(\"Bounding Box Shape: {}\".format(bboxes.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 0\n",
    "# origin: place the [0, 0] index of the array in the upper left or lower left corner of the axes\n",
    "# extent: floats(left, right, bottom, top)\n",
    "plt.imshow(imgs[i].T, cmap = 'Greys', interpolation = 'none', origin = 'lower', extent = [0, img_size, 0, img_size])\n",
    "# tranverse(.T) and flip upside down(origin = 'lower') to fit in the bounding box\n",
    "for bbox in bboxes[i]:\n",
    "    plt.gca().add_patch(matplotlib.patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], lw = 2, ec = 'r', fc = 'none'))\n",
    "    # patches.Rectangle((x, y), w, h, linewidth, edgecolor, facecolor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 64), 9.796607969292381e-18, 1.0000000000000004)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape and normalize the image data to mean 0 and std 1. \n",
    "X = (imgs.reshape(num_imgs, -1) - np.mean(imgs)) / np.std(imgs)\n",
    "X.shape, np.mean(X), np.std(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 4), 0.281514375, 0.17510010649157062)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize x, y, w, h by img_size, so that all values are between 0 and 1.\n",
    "# Important: Do not shift to negative values (e.g. by setting to mean 0), because the IOU calculation needs positive w and h.\n",
    "y = bboxes.reshape(num_imgs, -1) / img_size\n",
    "y.shape, np.mean(y), np.std(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and test.\n",
    "i = int(0.8 * num_imgs)\n",
    "train_X, test_X = X[:i], X[i:]\n",
    "train_y ,test_y = y[:i], y[i:]\n",
    "test_imgs = imgs[i:]\n",
    "test_bboxes = bboxes[i:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 200)               13000     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 804       \n",
      "=================================================================\n",
      "Total params: 13,804\n",
      "Trainable params: 13,804\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "model = Sequential([\n",
    "        Dense(200, input_dim = X.shape[-1]), \n",
    "        Activation('relu'), \n",
    "        Dropout(0.2), \n",
    "        Dense(y.shape[-1])\n",
    "        ])\n",
    "model.compile(loss = 'mse', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1250/1250 [==============================] - 1s 797us/step - loss: 0.0250 - accuracy: 0.7100 - val_loss: 7.9200e-04 - val_accuracy: 0.8539\n",
      "Epoch 2/30\n",
      "1250/1250 [==============================] - 1s 804us/step - loss: 0.0023 - accuracy: 0.8431 - val_loss: 3.0553e-04 - val_accuracy: 0.8710\n",
      "Epoch 3/30\n",
      "1250/1250 [==============================] - 1s 799us/step - loss: 0.0014 - accuracy: 0.8497 - val_loss: 1.6850e-04 - val_accuracy: 0.8535\n",
      "Epoch 4/30\n",
      "1250/1250 [==============================] - 1s 818us/step - loss: 0.0011 - accuracy: 0.8474 - val_loss: 1.5755e-04 - val_accuracy: 0.8343\n",
      "Epoch 5/30\n",
      "1250/1250 [==============================] - 1s 770us/step - loss: 9.3241e-04 - accuracy: 0.8464 - val_loss: 1.5366e-04 - val_accuracy: 0.8650\n",
      "Epoch 6/30\n",
      "1250/1250 [==============================] - 1s 822us/step - loss: 8.5262e-04 - accuracy: 0.8489 - val_loss: 1.3675e-04 - val_accuracy: 0.8509\n",
      "Epoch 7/30\n",
      "1250/1250 [==============================] - 1s 870us/step - loss: 8.0259e-04 - accuracy: 0.8468 - val_loss: 1.5564e-04 - val_accuracy: 0.8214\n",
      "Epoch 8/30\n",
      "1250/1250 [==============================] - 1s 844us/step - loss: 7.6912e-04 - accuracy: 0.8479 - val_loss: 2.3725e-04 - val_accuracy: 0.8473\n",
      "Epoch 9/30\n",
      "1250/1250 [==============================] - 1s 798us/step - loss: 7.2980e-04 - accuracy: 0.8464 - val_loss: 1.4849e-04 - val_accuracy: 0.9046\n",
      "Epoch 10/30\n",
      "1250/1250 [==============================] - 1s 751us/step - loss: 7.0079e-04 - accuracy: 0.8488 - val_loss: 1.3399e-04 - val_accuracy: 0.8938\n",
      "Epoch 11/30\n",
      "1250/1250 [==============================] - 1s 820us/step - loss: 6.9650e-04 - accuracy: 0.8501 - val_loss: 1.8968e-04 - val_accuracy: 0.8212\n",
      "Epoch 12/30\n",
      "1250/1250 [==============================] - 1s 749us/step - loss: 6.7544e-04 - accuracy: 0.8495 - val_loss: 1.9153e-04 - val_accuracy: 0.8250\n",
      "Epoch 13/30\n",
      "1250/1250 [==============================] - 1s 753us/step - loss: 6.4542e-04 - accuracy: 0.8507 - val_loss: 1.4337e-04 - val_accuracy: 0.9219\n",
      "Epoch 14/30\n",
      "1250/1250 [==============================] - 1s 825us/step - loss: 6.3968e-04 - accuracy: 0.8484 - val_loss: 1.2030e-04 - val_accuracy: 0.8827\n",
      "Epoch 15/30\n",
      "1250/1250 [==============================] - 1s 754us/step - loss: 6.2639e-04 - accuracy: 0.8487 - val_loss: 2.1873e-04 - val_accuracy: 0.8574\n",
      "Epoch 16/30\n",
      "1250/1250 [==============================] - 1s 843us/step - loss: 6.1256e-04 - accuracy: 0.8472 - val_loss: 1.4224e-04 - val_accuracy: 0.8364\n",
      "Epoch 17/30\n",
      "1250/1250 [==============================] - 1s 796us/step - loss: 6.1477e-04 - accuracy: 0.8511 - val_loss: 1.1657e-04 - val_accuracy: 0.8421\n",
      "Epoch 18/30\n",
      "1250/1250 [==============================] - 1s 764us/step - loss: 6.1319e-04 - accuracy: 0.8493 - val_loss: 1.4746e-04 - val_accuracy: 0.8021\n",
      "Epoch 19/30\n",
      "1250/1250 [==============================] - 1s 760us/step - loss: 6.0590e-04 - accuracy: 0.8499 - val_loss: 1.7821e-04 - val_accuracy: 0.8727\n",
      "Epoch 20/30\n",
      "1250/1250 [==============================] - 1s 748us/step - loss: 6.0055e-04 - accuracy: 0.8491 - val_loss: 1.2646e-04 - val_accuracy: 0.9048\n",
      "Epoch 21/30\n",
      "1250/1250 [==============================] - 1s 777us/step - loss: 5.9580e-04 - accuracy: 0.8493 - val_loss: 1.6704e-04 - val_accuracy: 0.8340\n",
      "Epoch 22/30\n",
      "1250/1250 [==============================] - 1s 761us/step - loss: 5.9808e-04 - accuracy: 0.8489 - val_loss: 2.0755e-04 - val_accuracy: 0.8697\n",
      "Epoch 23/30\n",
      "1250/1250 [==============================] - 1s 758us/step - loss: 5.9116e-04 - accuracy: 0.8496 - val_loss: 1.2585e-04 - val_accuracy: 0.8165\n",
      "Epoch 24/30\n",
      "1250/1250 [==============================] - 1s 763us/step - loss: 5.8880e-04 - accuracy: 0.8478 - val_loss: 1.6030e-04 - val_accuracy: 0.8193\n",
      "Epoch 25/30\n",
      "1250/1250 [==============================] - 1s 779us/step - loss: 5.9570e-04 - accuracy: 0.8495 - val_loss: 1.5794e-04 - val_accuracy: 0.8605\n",
      "Epoch 26/30\n",
      "1250/1250 [==============================] - 1s 742us/step - loss: 5.9268e-04 - accuracy: 0.8468 - val_loss: 1.0628e-04 - val_accuracy: 0.8151\n",
      "Epoch 27/30\n",
      "1250/1250 [==============================] - 1s 798us/step - loss: 5.8806e-04 - accuracy: 0.8480 - val_loss: 1.2764e-04 - val_accuracy: 0.8789\n",
      "Epoch 28/30\n",
      "1250/1250 [==============================] - 1s 811us/step - loss: 5.8316e-04 - accuracy: 0.8492 - val_loss: 1.2310e-04 - val_accuracy: 0.8624\n",
      "Epoch 29/30\n",
      "1250/1250 [==============================] - 1s 782us/step - loss: 5.9017e-04 - accuracy: 0.8467 - val_loss: 1.5073e-04 - val_accuracy: 0.8418\n",
      "Epoch 30/30\n",
      "1250/1250 [==============================] - 1s 738us/step - loss: 5.8334e-04 - accuracy: 0.8473 - val_loss: 1.6978e-04 - val_accuracy: 0.8467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22c00020af0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model.fit(train_X, train_y, epochs = 30, validation_data = (test_X, test_y), verbose = 1,\n",
    "          callbacks = [ModelCheckpoint('Single_Rectangle_model.h5',\n",
    "                                       monitor = 'val_accuracy',\n",
    "                                       save_best_only = True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-43dc15ee4f17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Single_Rectangle_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpred_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mpred_bboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_y\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mpred_bboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_bboxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_bboxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_X' is not defined"
     ]
    }
   ],
   "source": [
    "# Predict bounding boxes on the test images.\n",
    "from keras.models import load_model\n",
    "model_test = load_model('Single_Rectangle_model.h5')\n",
    "prediction = model_test.predict(test_X)\n",
    "pred_bboxes = prediction * img_size\n",
    "pred_bboxes = pred_bboxes.reshape(len(pred_bboxes), num_objects, -1)\n",
    "pred_bboxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IOU(bbox1, bbox2):\n",
    "    '''Calculate overlap between two bounding boxes [x, y, w, h] as the area of intersection over the area of unity'''\n",
    "    x1, y1, w1, h1 = bbox1[0], bbox1[1], bbox1[2], bbox1[3]\n",
    "    x2, y2, w2, h2 = bbox2[0], bbox2[1], bbox2[2], bbox2[3]\n",
    "\n",
    "    w_I = min(x1 + w1, x2 + w2) - max(x1, x2)\n",
    "    h_I = min(y1 + h1, y2 + h2) - max(y1, y2)\n",
    "    if w_I <= 0 or h_I <= 0:  # no overlap\n",
    "        return 0.\n",
    "    I = w_I * h_I\n",
    "\n",
    "    U = w1 * h1 + w2 * h2 - I\n",
    "\n",
    "    return I / U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x216 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show a few images and predicted bounding boxes from the test dataset. \n",
    "plt.figure(figsize = (12, 3))\n",
    "for i_subplot in range(1, 5):\n",
    "    plt.subplot(1, 4, i_subplot)\n",
    "    i = np.random.randint(len(test_imgs))\n",
    "    plt.imshow(test_imgs[i].T, cmap = 'Greys', interpolation = 'none', origin = 'lower', extent = [0, img_size, 0, img_size])\n",
    "    for pred_bbox, exp_bbox in zip(pred_bboxes[i], test_bboxes[i]):\n",
    "        plt.gca().add_patch(matplotlib.patches.Rectangle((pred_bbox[0], pred_bbox[1]), pred_bbox[2], pred_bbox[3], ec = 'r', fc = 'none'))\n",
    "        plt.annotate(\"IOU: {:.2f}\".format(IOU(pred_bbox, exp_bbox)), (pred_bbox[0], pred_bbox[1] + pred_bbox[3] + 0.2), color = 'r')\n",
    "        # plt.annotate(text, (x, y), color)\n",
    "        \n",
    "plt.savefig(\"single-rectangle_prediction.png\", dpi = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8082458046829537"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean IOU (overlap) between the predicted and expected bounding boxes on the test dataset. \n",
    "summed_IOU = 0.\n",
    "for pred_bbox, test_bbox in zip(pred_bboxes.reshape(-1, 4), test_bboxes.reshape(-1, 4)):\n",
    "    summed_IOU += IOU(pred_bbox, test_bbox)\n",
    "mean_IOU = summed_IOU / len(pred_bboxes)\n",
    "mean_IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
